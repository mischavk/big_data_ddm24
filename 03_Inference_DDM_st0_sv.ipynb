{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d7c94654",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-12 11:01:59.624563: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-08-12 11:01:59.646091: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-08-12 11:01:59.972740: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/home/mischa/miniconda3/envs/bf/lib/python3.10/site-packages/bayesflow/trainers.py:27: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import scipy\n",
    "from numba import njit, prange\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import bayesflow as bf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from multiprocessing import Pool\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "import pandas as pd \n",
    "from sklearn.covariance import EmpiricalCovariance\n",
    "import priors_and_simulators as ps\n",
    "\n",
    "# Suppress scientific notation for floats\n",
    "np.set_printoptions(suppress=True)\n",
    "RNG = np.random.default_rng(2023)\n",
    "\n",
    "#Settings\n",
    "# Path to data\n",
    "PATH = '/home/mischa/Documents/bayesflow/prj_real_life_ddm/data/prepared_data/'\n",
    "\n",
    "# Where to save files\n",
    "PATH_TO_SAVE = '/home/mischa/Documents/bayesflow/prj_real_life_ddm/data/pickle_st0_sv/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7fa475e",
   "metadata": {},
   "source": [
    "# Load neural networks from checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2991594f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Performing 2 pilot runs with the DDM model...\n",
      "INFO:root:Shape of parameter batch after 2 pilot simulations: (batch_size = 2, 8)\n",
      "INFO:root:Shape of simulation batch after 2 pilot simulations: (batch_size = 2, 120, 4)\n",
      "INFO:root:No optional prior non-batchable context provided.\n",
      "INFO:root:No optional prior batchable context provided.\n",
      "INFO:root:No optional simulation non-batchable context provided.\n",
      "INFO:root:No optional simulation batchable context provided.\n",
      "2024-08-12 11:02:15.197235: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-08-12 11:02:15.225834: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-08-12 11:02:15.225923: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-08-12 11:02:15.227140: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-08-12 11:02:15.227209: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-08-12 11:02:15.227254: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-08-12 11:02:15.525494: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-08-12 11:02:15.525582: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-08-12 11:02:15.525629: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-08-12 11:02:15.525674: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 7531 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3080, pci bus id: 0000:01:00.0, compute capability: 8.6\n",
      "INFO:root:Loaded loss history from ddm_model_st0_sv_net2/history_200.pkl.\n",
      "INFO:root:Networks loaded from ddm_model_st0_sv_net2/ckpt-200\n",
      "INFO:root:Performing a consistency check with provided components...\n",
      "2024-08-12 11:02:16.936267: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:606] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "INFO:root:Done.\n"
     ]
    }
   ],
   "source": [
    "# Get network settings\n",
    "\n",
    "PARAM_NAMES = [\n",
    "    r\"$v_{congruent}$\",\n",
    "    r\"$v_{incongruent}$\",\n",
    "    r\"$a_{congruent}$\",\n",
    "    r\"$a_{incongruent}$\",\n",
    "    r\"$\\tau_{correct}$\",\n",
    "    r\"$\\tau_{error}$\",\n",
    "    r\"$s_{tau}$\",\n",
    "    r\"$s_{v}$\"\n",
    "]\n",
    "\n",
    "prior = bf.simulation.Prior(prior_fun=ps.sv_st0_ddm_prior_fun, param_names=PARAM_NAMES)\n",
    "\n",
    "prior_means, prior_stds = prior.estimate_means_and_stds(n_draws=10000)\n",
    "prior_means = np.round(prior_means, decimals=1)\n",
    "prior_stds = np.round(prior_stds, decimals=1)\n",
    "\n",
    "simulator = bf.simulation.Simulator(simulator_fun=ps.sv_st0_ddm_simulator_fun)\n",
    "\n",
    "model = bf.simulation.GenerativeModel(prior=prior, simulator=simulator, name=\"DDM\")\n",
    "\n",
    "def configurator(forward_dict):\n",
    "    \"\"\"Configure the output of the GenerativeModel for a BayesFlow setup.\"\"\"\n",
    "\n",
    "    out_dict = {}\n",
    "    out_dict[\"summary_conditions\"] = forward_dict[\"sim_data\"]\n",
    "    params = forward_dict[\"prior_draws\"].astype(np.float32)\n",
    "    # Standardize parameters\n",
    "    out_dict[\"parameters\"] = (params - prior_means) / prior_stds\n",
    "    \n",
    "    return out_dict\n",
    "\n",
    "summary_net = bf.networks.SetTransformer(input_dim=4, summary_dim=30, name=\"ddm_summary\")\n",
    "\n",
    "inference_net = bf.networks.InvertibleNetwork(\n",
    "    num_params=len(prior.param_names),\n",
    "    coupling_settings={\"dense_args\": dict(kernel_regularizer=None), \"dropout\": False},\n",
    "    name=\"ddm_inference\")\n",
    "\n",
    "amortizer = bf.amortizers.AmortizedPosterior(inference_net, summary_net, name=\"ddm_amortizer\",\n",
    "                                            summary_loss_fun='MMD')\n",
    "\n",
    "trainer = bf.trainers.Trainer(\n",
    "        generative_model=model, amortizer=amortizer, configurator= configurator,\n",
    "    checkpoint_path=\"ddm_model_st0_sv_net2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f4613e3",
   "metadata": {},
   "source": [
    "# Run amortized inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f04e5a41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prepared_False2004iat.p loaded\n",
      "0.04% improper samples rejected\n",
      "prepared_False2004iat.p inference done\n",
      "0 people with <1000 proper samples (for at least one parameter) excluded\n",
      "prepared_False2004iat.p Mahalanobis check done\n",
      "prepared_False2004iat.p saving done\n",
      "prepared_False2005iat.p loaded\n",
      "0.045% improper samples rejected\n",
      "prepared_False2005iat.p inference done\n",
      "0 people with <1000 proper samples (for at least one parameter) excluded\n",
      "prepared_False2005iat.p Mahalanobis check done\n",
      "prepared_False2005iat.p saving done\n"
     ]
    }
   ],
   "source": [
    "#This is where the magic happens\n",
    "\n",
    "# 1. Store all data-set chunk names in a list\n",
    "datasets = sorted(os.listdir(PATH))[1:3]\n",
    "\n",
    "# 2. For each chunk\n",
    "for dataset_name in datasets:\n",
    "    \n",
    "        # 2.1 Load chunk\n",
    "        loaded_pickle = pickle.load(open(PATH +str(dataset_name), \"rb\" ))\n",
    "        \n",
    "        X_test = loaded_pickle['data_array']\n",
    "        y_test = loaded_pickle['outcome_array']\n",
    "        rt_summaries = loaded_pickle['rt_summaries']\n",
    "        \n",
    "        print(str(dataset_name)+ \" loaded\")\n",
    "\n",
    "\n",
    "        # 2.2 Estimate chunk\n",
    "\n",
    "        samples_dm = np.concatenate([amortizer.sample(input_dict = {\"summary_conditions\": x}, n_samples=3000,\n",
    "                                                  to_numpy=True) for x in np.array_split(X_test, 50)], axis=0)\n",
    "\n",
    "        samples_dm = samples_dm * prior_stds + prior_means\n",
    "        \n",
    "        # Discard negative samples for positively bounded parameters\n",
    "        samples_dm[:,:,2:8][samples_dm[:,:,2:8]<0] = np.nan  \n",
    "        \n",
    "        print(str(np.round(np.sum(np.isnan(samples_dm))/\n",
    "                           (np.sum(np.isnan(samples_dm))+np.sum(~np.isnan(samples_dm))),\n",
    "                           5)*100) +\"% improper samples rejected\")\n",
    "        \n",
    "        print(str(dataset_name)+ \" inference done\")\n",
    "\n",
    "        # 2.3 Compute summaries of parameter posteriors: means, medians, stds, Q0.025, Q0.0975\n",
    "        \n",
    "        estimates = ps.compute_summaries(samples_dm)\n",
    "        \n",
    "        # Exclude people with less than 1000 proper posterior samples for at least one parameter\n",
    "        estimates[np.sum((np.sum(np.isnan(samples_dm), axis=1)>2000), axis=1)>0,:] = np.nan \n",
    "        \n",
    "        print(str(np.sum((np.sum(np.isnan(samples_dm), axis=1))>2000)) +\n",
    "              \" people with <1000 proper samples (for at least one parameter) excluded\")\n",
    "\n",
    "        # 2.4 Get empirical Mahalanobis distances for summary statistics provided by network\n",
    "            \n",
    "        summary_statistics_empirical = np.concatenate([trainer.amortizer.summary_net (x)\n",
    "                                                       for x in np.array_split(X_test, 50)], axis=0)\n",
    "        \n",
    "        cov = EmpiricalCovariance().fit(summary_statistics_empirical)\n",
    "\n",
    "        mahalanobis_empirical = cov.mahalanobis(summary_statistics_empirical)\n",
    "        \n",
    "        print(str(dataset_name)+ \" Mahalanobis check done\")\n",
    "\n",
    "        \n",
    "        # 2.5 Store everything together (serialized, pickle.dump) as a dict with keys \n",
    "        dict_to_store = {'data_array': X_test, 'est_array': estimates, \"outcome_array\": y_test,\n",
    "                        'mahalanobis': mahalanobis_empirical, 'rt_summaries': rt_summaries}\n",
    "        pickle.dump(dict_to_store,\n",
    "                    open(PATH_TO_SAVE +\"estimates_\" +str(dataset_name),\"wb\"))\n",
    "        print(str(dataset_name)+ \" saving done\")\n",
    "\n",
    "# 3. Celebrate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2de16f6",
   "metadata": {},
   "source": [
    "# Create csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "15d80a9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "estimates_prepared_False2005iat.p done\n",
      "estimates_prepared_False2004iat.p done\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame()\n",
    "datasets = os.listdir(PATH_TO_SAVE)\n",
    "\n",
    "for dataset in datasets:\n",
    "        pickles = pickle.load(open(PATH_TO_SAVE +str(dataset), \"rb\" ))\n",
    "        df_oneset = np.concatenate((pickles['est_array'],pickles['outcome_array'],pickles['rt_summaries'],\n",
    "                                   np.expand_dims(pickles['mahalanobis'], axis=1)), axis=1)\n",
    "        df = pd.concat([df, pd.DataFrame(data = df_oneset)])\n",
    "        df[\"dataset\"] = str(dataset)      \n",
    "        \n",
    "        print(str(dataset)+\" done\")\n",
    "\n",
    "df.columns = [\"v_congruent\", \"v_incongruent\", \"a_congruent\", \"a_incongruent\",\n",
    "                 \"tplus\", \"tminus\", \"st0\", \"sv\",\n",
    "                \"v_congruent_median\", \"v_incongruent_median\", \"a_congruent_median\", \"a_incongruent_median\",\n",
    "                 \"tplus_median\",\"tminus_median\", \"st0_median\", \"sv_median\",\n",
    "                \"v_congruent_std\", \"v_incongruent_std\", \"a_congruent_std\", \"a_incongruent_std\",\n",
    "                 \"tplus_std\",\"tminus_std\", \"st0_std\", \"sv_std\",\n",
    "                \"v_congruent_q025\", \"v_incongruent_q025\", \"a_congruent_q025\", \"a_incongruent_q025\",\n",
    "                 \"tplus_q025\",\"tminus_q025\", \"st0_q025\", \"sv_q025\",\n",
    "                \"v_congruent_q975\", \"v_incongruent_q975\", \"a_congruent_q975\", \"a_incongruent_q975\",\n",
    "                 \"tplus_q975\",\"tminus_q975\", \"st0_q975\", \"sv_q975\",\n",
    "              \n",
    "                  \"session_id\", \"age\",\n",
    "              \n",
    "                 \"congruent_rt_correct\", \"congruent_rt_correct_sd\",\n",
    "                \"congruent_rt_error\", \"congruent_rt_error_sd\", \"congruent_accuracy\",\n",
    "                \"incongruent_rt_correct\", \"incongruent_rt_correct_sd\",\n",
    "                  \"incongruent_rt_error\", \"incongruent_rt_error_sd\", \"incongruent_accuracy\",\n",
    "                \"word_rt_correct\", \"word_rt_correct_sd\",\n",
    "                \"word_rt_error\",\"word_rt_error_sd\", \"word_accuracy\",\n",
    "                \"picture_rt_correct\", \"picture_rt_correct_sd\", \n",
    "               \"picture_rt_error\", \"picture_rt_error_sd\", \"picture_accuracy\",\n",
    "              \n",
    "                 \"mahalanobis_distance\", \"dataset\"]\n",
    "\n",
    "df.to_csv(\"df_sv_st0_ddm.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
